---
title: 'Perturbseq中细胞系的异质性如何去除'
date: 2025-03-19
permalink: /posts/2025/03/blog-post-3/
tags:
  - perturbseq analysis
  - python
  - cell line, heterogeneity
---

perturbseq的数据在分析的时候，发现所有的细胞（敲除的、没敲除的）在umap上混在一起。本来期待的敲除对细胞造成的表达量的影响，却不是最终观测到的最大的差别，那么这些差别是什么呢？

细胞周期的影响
---
细胞系在培养的时候，细胞处于不同的细胞周期阶段，细胞周期的差异，是细胞背景差异的原因之一。

细胞的异质性
---
因为使用的是细胞系，细胞系中细胞之间的差异可能比较大，尤其是在实验过程中，添加实验条件之后连续培养多天，经过这段时间的增殖之后，组内细胞之间的差异可能会更大。

深度学习的方法去除
---
目前看见的两个方法
1. contrastiveVI(使用变分自编码器，学习none-perturbed control cell的内部差异作为背景，在细胞间的比较时去除该差异)
2. scCAPE(使用变分自编码器，学习none-perturbed control cell的内部差异作为背景，在细胞间的比较时去除该差异)

最近看了contrastiveVI的核心代码。contrastiveVI基于scVI的框架，其中decoder的部分是直接使用的scVI库中的decoder类。我们可以从scVI和contrastiveVI的文章看出来，这两种方法都是基于零膨胀负二项分布的单细胞counts数据生成模型。scVI的教程中有使用pyro构建新模型的部分，因此，我也尝试了一下使用pyro框架进行contrastiveVI的复现。以下是代码。

'''python
import pyro
import scanpy as sc
from scanpy import AnnData
import scvi
import torch
import torch.nn as nn
import numpy as np
import pyro.distributions as dist
import pandas as pd
import pyro
from pyro.infer import SVI, Trace_ELBO
from pyro.optim import Adam
from torch.utils.data import  DataLoader
import scipy
torch.set_default_tensor_type(torch.DoubleTensor)

class FCNN(nn.Module):
    def __init__(self, n_input, n_output, n_hidden=128, n_layers=1, dropout_rate=0.1, transformer=None):
        super(FCNN, self).__init__()
        self.n_input = n_input
        self.n_output = n_output
        self.n_hidden = n_hidden
        self.n_layers = n_layers
        self.dropout_rate = dropout_rate

        self.layers = nn.ModuleList()
        self.layers.append(nn.Linear(n_input, n_hidden))
        self.layers.append(nn.ReLU())
        self.layers.append(nn.Dropout(dropout_rate))
        for i in range(n_layers - 1):
            self.layers.append(nn.Linear(n_hidden, n_hidden))
            self.layers.append(nn.ReLU())
            self.layers.append(nn.Dropout(dropout_rate))
        self.layers.append(nn.Linear(n_hidden, n_output))
        if transformer is not None:
            self.transformer = transformer
        else:
            self.transformer = None
    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        if self.transformer == 'exp':
            x = torch.exp(x)
        elif self.transformer == 'sigmoid':
            x = torch.sigmoid(x)
        elif self.transformer == 'softplus':
            x = torch.nn.functional.softplus(x)
        elif self.transformer == 'tanh':
            pass
        #elif self.transformer == 'clamp':
        #    x = torch.clamp(x, -50, 50)
        return x


class PyroVAE( scvi.module.base.PyroBaseModuleClass):
    def __init__(self, n_latent_t, n_latent_c , n_hidden=200, n_layers=1, dropout_rate=0.1, use_library_size=True, control_label='Control'):
        super(PyroVAE, self).__init__()
        self.control_label = control_label
        self.use_library_size = use_library_size
        self.n_latent_t = n_latent_t
        self.n_latent_c = n_latent_c
        self.n_input = PyroVAE.n_genes
        self.n_hidden = n_hidden
        self.n_layers = n_layers
        self.dropout_rate = dropout_rate

        self.encoder_t_m = FCNN(self.n_input, n_latent_t, n_hidden=n_hidden, n_layers=n_layers, dropout_rate=dropout_rate)  ## encoder for target cells
        self.encoder_t_v = FCNN(self.n_input, n_latent_t, n_hidden, n_layers, dropout_rate, 'exp')  ## encoder for target cells

        self.encoder_c_m = FCNN(self.n_input, n_latent_c, n_hidden, n_layers, dropout_rate)  ## encoder for background cells
        self.encoder_c_v = FCNN(self.n_input, n_latent_c, n_hidden, n_layers, dropout_rate, 'exp' )  ## encoder for background cells
        n_Rho_input = n_latent_t + n_latent_c + self.n_covariates
        
        n_latent = n_latent_t + n_latent_c + self.n_covariates
        self.decoder_Rho = FCNN(n_Rho_input, self.n_input, n_hidden, n_layers, dropout_rate,'sigmoid')  ## decoder for Probability of gene expression for each gene
        self.decoder_zero_gate = FCNN(n_latent,  1, n_hidden, n_layers, dropout_rate,'sigmoid') ## decoder for zero gates for each gene
        #self.decoder = DecoderSCVI( n_latent, self.n_input, n_layers, dropout_rate)



    def model(self, x, is_background, library_x, l_mu, l_sig, covariates):
        ## generative/decoding process
        pyro.module('pyroVAE', self)
        Theta = x.mean(0) / x.var(0)
        with pyro.plate('cell', x.shape[0]):
            z_t_loc = torch.zeros( torch.Size((x.shape[0],self.n_latent_t)))
            z_t_var = torch.ones( torch.Size((x.shape[0],self.n_latent_t)))
            z_t = pyro.sample('t', pyro.distributions.Normal(z_t_loc, z_t_var).to_event(1))
            z_t[list(is_background[:,0].nonzero()),:] = 0

            z_c_loc = torch.zeros(torch.Size((x.shape[0],self.n_latent_c)))
            z_c_var = torch.ones(torch.Size((x.shape[0],self.n_latent_c)))
            z_c = pyro.sample('c', pyro.distributions.Normal(z_c_loc, z_c_var).to_event(1))
            Rho = self.decoder_Rho( torch.cat((z_t, z_c, covariates), 1) ) 
            if not self.use_library_size:
                l =  dist.LogNormal(  covariates * l_mu.t  ,   covariates * (l_sig**2).t )   # matrix multiplication
            else:
                l = library_x
            print(l.shape, Rho.shape)
            #l = torch.squeeze(l, 1)
            print(l.shape, Rho.shape)
            zero_gate = self.decoder_zero_gate( torch.cat((z_t, z_c, covariates), 1) )

            print(Rho.shape, zero_gate.shape)
            pyro.sample('x', dist.ZeroInflatedNegativeBinomial( torch.exp(l).int() , probs = Rho, gate_logits=zero_gate).to_event(1), obs=x )

    def guide(self, x, is_background, library_x , l_mu, l_sig, covariates):
        ## encoding process
        pyro.module('pyroVAE', self)
        with pyro.plate('cell', x.shape[0]):
            x_ = torch.log(x + 1)
            z_t_m = self.encoder_t_m(x_)
            z_t_v = self.encoder_t_v(x_)

            z_c_m = self.encoder_t_m(x_)
            z_c_v = self.encoder_t_v(x_)

            pyro.sample('t', pyro.distributions.Normal(z_t_m, z_t_v).to_event(1))
            pyro.sample('c', pyro.distributions.Normal(z_c_m, z_c_v).to_event(1))


    def train(self, epoches=1000):
        pyro.clear_param_store()
        svi = SVI(self.model, self.guide, Adam({"lr": 0.001}), loss=Trace_ELBO())
        x = torch.tensor(self.adata.X.todense() if isinstance(self.adata.X, scipy.sparse.spmatrix) else self.adata.X, dtype=torch.float64) 
        library = torch.sum(x, dim=1, keepdim=True).log() ## library size
        l_mu = torch.mean(library, dim=0) ## mean of library size
        l_sig = torch.std(library, dim=0) ## standard deviation of library size
        is_background = torch.unsqueeze(torch.tensor(self.is_background),1)
        covariates_mat = torch.tensor(self.adata.obs[ self.covariates_col ].to_numpy(), dtype=torch.float64)
        dataset = torch.cat([x, library, is_background, covariates_mat], dim=1)
        dataloader = DataLoader(dataset, batch_size=128, shuffle=True)
        for epoch_i in range(epoches):
            i = 0
            for train_batch in dataloader:
                x = torch.index_select(train_batch, 1, torch.arange(0, self.n_input) )
                library = torch.index_select(train_batch, 1, torch.arange(self.n_input, self.n_input + 1) )
                is_background = torch.index_select(train_batch, 1, torch.arange(self.n_input + 1, self.n_input + 2) )
                covariates_mat = torch.index_select(train_batch, 1, torch.arange(self.n_input + 2, self.n_input + 2 + self.n_covariates) )
                loss = svi.step(x, is_background, library, l_mu, l_sig, covariates_mat)
                #if i %100 ==0:
                print('epoch: ', epoch_i, 'step: ', i, 'loss: ', loss)
                i += 1
        return svi


    @classmethod
    def set_up_anndata(cls, adata, covariates_col:list, background_col:str, background_label:str , layer='X'):
        if isinstance(adata, AnnData):
            adata = adata
        elif isinstance(adata, str):
            adata = sc.read(adata)
        if layer != 'X':
            adata.X = adata.layers[layer].copy()
        cls.n_covariates = len(covariates_col)
        cls.covariates_col = covariates_col
        cls.is_background = adata.obs[background_col].values == background_label
        cls.n_genes = adata.shape[1]
        cls.adata = adata
        cls.background_col = background_col
        cls.background_label = background_label



dir = '/home/yzeng/proj/Perturb_seq/rerun/'
adata=sc.read(dir+'Clean_Counts.h5ad')
adata.layers['counts'] = adata.X.copy()  # preserve counts
adata.obs['batch_wider'] = 1
df2 = pd.DataFrame(adata.obs[['batch','batch_wider']]).pivot(columns='batch').fillna('0')
for obs_add in df2['batch_wider'].columns:
    obs_n = 'batch' + obs_add
    adata.obs[obs_n] = df2['batch_wider'][obs_add].astype(float)
adata.var['mt'] = adata.var_names.str.startswith('MT-')  # annotate the group of mitochondrial genes as 'mt'
sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True)

sc.pp.highly_variable_genes(adata, layer='counts', flavor='seurat_v3', n_top_genes=2000, subset=True)

covariates_col = ['batch0','batch1','batch2', 'batch3','batch4','batch5','batch6','batch7','pct_counts_mt']
PyroVAE.set_up_anndata( adata, covariates_col, 'gene.compact', 'control' , 'counts' )
pyro_model = PyroVAE( 10, 10)
pyro_model.train( 2000)




'''
